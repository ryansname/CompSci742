\documentclass[10pt,conference]{IEEEtran}

\usepackage{enumitem}
\usepackage{graphics}
\usepackage{cite}
\usepackage{url}

\title{Title}
\author{\IEEEauthorblockN{Chelsea Farley, Ryan Lewis, David Armstrong, Rina Gao and Ryunosuke Madenokoji}
\IEEEauthorblockA{The University of Auckland}}
\date{Today}

\begin{document}
\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}
There has been an increase in the overall traffic volume on the Web. The rate of increase of data traffic has been shown to follow Moore's law \cite{williams05}. Moore's law as it pertains to data traffic, states that the overall traffic volume will double annually. This property of data traffic means that the scalability of the Web is more important than ever. Additionally, the time an average user will wait for a page to load has decreased from eight seconds in 2000 to three seconds in 2009 \cite{Butkiewicz}. Therefore, it is becoming paramount that optimisations are made to improve Web performance.

In order to improve the performance and scalability of the Web, we must first develop an understanding of current Web workload characteristics and how they have changed over time. An understanding of the evolution of workload characteristics is vital in the development of effective caching architectures.

The purpose of this paper is to revisit several of the invariants identified by Arlitt and Williamson \cite{keynote} and analyse if they remain true for a more recent Web server log.

In the following section we will discuss work related to our analysis and what our analysis contributes. In section \ref{methodology} we describe the characteristics we chose to analyse and the metrics and tools we used to perform this analysis. Then we will explain the results we recorded in section \ref{results} and explain the practical implications of these results in section \ref{discussion}. Finally, we will conclude our paper and examine potential future work in the area of web workload characterisation in section \ref{conclusions}.

\section{Related Work}\label{related_work}
There have been several studies investigating the characteristics of Web workloads. These studies can be useful in understanding the evolution of Web traffic under differing conditions. In order to improve the scalability and performance of the Internet, we must have a complete understanding of the different traffic flows it may have to withstand. We have summarised several previous studies to provide an overview of these traffic flows. 

Arlitt and Jin \cite{world_cup} analysed data from the 1994 World Cup website. This study was motivated by the high volume of users accessing the site, and therefore the potential to extrapolate the future characteristics of web traffic. They discovered that most users are interested in cacheable files and thus caching plays an important role in the scalability and performance of the Internet. They also found that most user sessions had only one request per session, and therefore managing how connections are closed may be improved by examining session properties.

In 1996 and 1997, Arlitt and Williamson \cite{keynote, invariants} analysed logs from six Web servers and presented their findings. The motivation of their study was to find invariants of Web workload characteristics across six server logs. These invariants would be useful in the development of techniques for improving caching and the general performance of the Web. They found 10 invariants which may be useful in future analyses of Web traffic. They also discovered that there is the opportunity for caching to improve performance, and more specifically that caching to reduce the number of requests may be more effective than caching to reduce bytes transferred. 
This study was superseded by a study by Faber et al. \cite{Faber} which looked into the original ten invariants, but also discovered three more. Their study concludes that many of the original ten invariants have remained relatively the same in the twenty years that have passed. Among the three new invariants discovered, one showed a significant difference in results between scientific and general websites.

Williams et al. \cite{williams05} produced an analysis of Web server logs from 2004 from the same three universities as the initial paper \cite{keynote}. The aim of the analysis was to uncover the impact of Moore's law on the ten invariants of the initial study. They concluded that despite the increase in traffic volume, most of the ten invariants remained unchanged. However, the percentage of successful requests and the percentage of HTML and image files transferred had decreased. 

In 2007, Gill et al. \cite{youtube} analysed YouTube usage on the University of Calgary network and collected statistics on global video popularity. The aim of this study was to investigate Web 2.0 workload characteristics to allow for improvements in network management, capacity planning and the design of new systems. Web 2.0 marks a shift towards user generated content and with it comes a plethora of metadata. It was found that although the concentration of references was reduced from that of traditional Web workloads \cite{keynote}, metadata should be used to improve the effectiveness of caching.

Cherkasova and Gupta \cite{Cherkasova} investigated client access patterns, media server access trends, and the evolution of media on websites over time. Their research was split into static and temporal analysis of the data. They looked at the locality of accesses, which was similar to that of a web server with 14\%-30\% of the server files accounting for 90\% of media sessions, and 92\%-94\% of the bytes transferred. 16\%-19\% of files were accessed only once, and the first five weeks of a file’s existence account for 70\%-80\% of its total accesses. They also looked into the trends associated with media session lengths, and the behavior of clients with incomplete sessions. Their research concentrated on comparing media server workloads with traditional web server workloads.

The study by Bai and Williamson in 2002 \cite{Bai}, discovers the request arrival process at each level of a multi-layer web proxy caching hierarchy using trace-driven simulation and artificial web workloads to analyse. The simulation results displayed that a web cache allows the decrease in both the peak and the mean request arrival time for the traffic workloads. Nonetheless, the request arrival process after cache may vary or stay the same depending on the input measures. Other results showed that the filtered request arrival procedure remained similar to themselves with reduced mean, and in a web caching hierarchy the Gamma distribution is suited for modelling the request arrival process.

In 2012, Tantithamthavorn and Rungsawang \cite{facebook} conducted a study on the Facebook usage at Kasetsart University to extract information and discovered new knowledge from a web traffic log. It was based on traffic and login history logs of the university’s network during a seven days period in March, 2011. The summarised HTTP sessions showed that there were over 25,000 IP addresses with nearly 40,000 distinct users. The study resulted in a pattern analysis in aspects such as the time spent communicating online, distribution of HTTP, Facebook traffic workload analysis, and more. The results showed that Zipf’s law was diplayed at the distribution of HTTP Requests at the level of hostname, Facebook.com has the most HTTP requests, 90\% of all sessions spent in total up to 24 hours, and more. Overall the study showed a typical web workload at a university.

Varnagar et al. \cite{Varnagar} investigated the patterns and traits of the World Wide Web (WWW) by analysing the access patterns which are captured in a form of web log files. The methods such as application of clustering, classification, association rule, and sequential patterns were used to extract useful knowledge. The results from the study exhibited the usefulness of the methods used to obtain new pattern and knowledge.

Almeida et al. \cite{almeida} analysed the performance of a PC acting as dedicated Web server. The purpose of their study was to measure the activity and resource consumption of the operating system using WebMonitor. This provided insight into variances in the performance of servicing web requests. They observed similar user session properties to Arlitt and Jin \cite{world_cup} as most requests required a new connection, which may be problematic for operating systems that are not designed to cope with this. This highlighted the importance of operating system and network protocol implementation.

Almeida et al. \cite{reference_locality} examined models for gathering data on spatial and temporal locality. The motivation of this study was the insufficiency of simple popularity based models being insufficient for capturing spatial and temporal locality. They discovered that for accesses following a temporal access pattern, caching is beneficial in improving performance. Whereas, for accesses following a spatial access pattern, prefetching is beneficial in improving performance.

\section{Methodology}\label{methodology}
\subsection{Characteristics}\label{lab:characteristics}
\subsection{Metrics}
To determine which characteristics from section \ref{lab:characteristics} were present we decided on the following metrics:
\begin{itemize}[noitemsep]
    \item IpCollector
    \item SuccessCollector
    \item MeanTransferCollector
    \item ProgressReporter
    \item OneTimeReferenceCollector
    \item CacheCollector
    \item CachedBytes
    \item TotalTransferCollector
    \item ReferenceConcentrationCollector
    \item AccessTimeCollector
\end{itemize}
\subsection{Tools}

\section{Results}\label{results}

\section{Discussion}\label{discussion}

\section{Conclusions and Future Work}\label{conclusions}

\bibliographystyle{IEEEtran}
\bibliography{references.bib}
\end{document}